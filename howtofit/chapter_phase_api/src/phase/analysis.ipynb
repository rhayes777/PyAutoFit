{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af\n",
        "from src.dataset import dataset as ds\n",
        "from src.fit import fit as f\n",
        "from src.phase import visualizer\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\"\"\"\n",
        "The `analysis.py` module contains the Analysis class we introduced in chapter 1. Whereas before this class took the\n",
        "data and noise-map separately, we now pass it an instance of the `Dataset` class. We have also restructured the code\n",
        "performing the model-fit, so that we can reuse it in the `result.py` module.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, dataset: ds.Dataset, settings):\n",
        "        \"\"\"The `Dataset` is created in `phase.py`.\"\"\"\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.settings = settings\n",
        "\n",
        "        self.visualizer = visualizer.Visualizer()\n",
        "\n",
        "    def log_likelihood_function(self, instance: af.ModelInstance) -> float:\n",
        "        \"\"\"\n",
        "        Returns the fit of a list of Profiles (Gaussians, Exponentials, etc.) to the dataset, using a\n",
        "        model instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance : af.ModelInstance\n",
        "            The list of Profile model instance (e.g. the Gaussians, Exponentials, etc.).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fit : Fit.log_likelihood\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        In chapter 1, the `instance` that came into the `log_likelihood_function` when using multiple profiles was a \n",
        "        `CollectionPriorModel`. We accessed the attributes of this instance as follows:\n",
        "        \n",
        "            print(instance.gaussian.sigma)\n",
        "            model_data = instance.gaussian.profile_from_xvalues(xvalues=self.dataset.xvalues)\n",
        "\n",
        "        In `phase.py` this instance is now set up as a `phase_property`, as seen by the following line of Python code:\n",
        "         \n",
        "        profiles = af.PhaseProperty(\"profiles\")\n",
        "         \n",
        "        This means the `instance` input into the `log_likelihood_function` has an additional dictionary containing the\n",
        "        profiles:\n",
        "\n",
        "            print(instance.profiles.gaussian.sigma)\n",
        "            model_data = instance.profiles.gaussian.profile_from_xvalues(xvalues=self.dataset.xvalues)\n",
        "\n",
        "        The names of the attributes of the `profiles` dictionary again correspond to the inputa of the \n",
        "        `CollectionPriorModel`. Lets look at a second example:\n",
        "\n",
        "            model = CollectionPriorModel(\n",
        "                          gaussian_0=profiles.Gaussian,\n",
        "                          gaussian_1=profiles.Gaussian,\n",
        "                          whatever_i_want=profiles.Exponential\n",
        "                     ).\n",
        "\n",
        "            print(instance.profiles.gaussian_0)\n",
        "            print(instance.profiles.gaussian_1)\n",
        "            print(instance.profiles.whatever_i_want.centre)\n",
        "\n",
        "        In this example project, we only have one phase property, `profiles`, making this additional dictionary appear\n",
        "        superflous. However, one can imagine that for complex model-fitting problems we might have many phase \n",
        "        properties, and this ability to separate them will make for cleaner and more manageable source code!\n",
        "        \"\"\"\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "        fit = self.fit_from_model_data(model_data=model_data)\n",
        "        return fit.log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance: af.ModelInstance) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a list comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "\n",
        "        Note how we now use `instance.profiles` to get this dictionary, where in chapter ` we simply used `instance`.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance : af.ModelInstance\n",
        "            The list of Profile model instance (e.g. the Gaussians, Exponentials, etc.).\n",
        "        \"\"\"\n",
        "\n",
        "        return sum(\n",
        "            [\n",
        "                profile.profile_from_xvalues(xvalues=self.dataset.xvalues)\n",
        "                for profile in instance.profiles\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def fit_from_model_data(self, model_data: np.ndarray) -> f.FitDataset:\n",
        "        \"\"\"\n",
        "        Call the `FitDataset` class in `fit.py` to create an instance of the fit, whose `log_likelihood` property\n",
        "        is used in the `log_likelihood_function`.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_data : np.ndarray\n",
        "            The model data of the 1D profile(s) the data is fitted with.\n",
        "        \"\"\"\n",
        "\n",
        "        return f.FitDataset(dataset=self.dataset, model_data=model_data)\n",
        "\n",
        "    def visualize(\n",
        "        self, paths: af.Paths, instance: af.ModelInstance, during_analysis: bool\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        This visualize function is used in the same fashion as it was in chapter 1. The `Visualizer` class is described\n",
        "        in tutorial 2 of this chapter.\n",
        "        \"\"\"\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "        fit = self.fit_from_model_data(model_data=model_data)\n",
        "\n",
        "        self.visualizer.visualize_dataset(paths=paths, dataset=self.dataset)\n",
        "        self.visualizer.visualize_fit(\n",
        "            paths=paths, fit=fit, during_analysis=during_analysis\n",
        "        )\n",
        "\n",
        "    def save_attributes_for_aggregator(self, paths):\n",
        "        \"\"\"Save files like the dataset, mask and settings as pickle files so they can be loaded in the ``Aggregator``\"\"\"\n",
        "\n",
        "        # These functions save the objects we will later access using the aggregator. They are saved via the `pickle`\n",
        "        # module in Python, which serializes the data on to the hard-disk.\n",
        "\n",
        "        with open(f\"{paths.pickle_path}/dataset.pickle\", \"wb\") as f:\n",
        "            pickle.dump(self.dataset, f)\n",
        "\n",
        "        with open(f\"{paths.pickle_path}/settings.pickle\", \"wb+\") as f:\n",
        "            pickle.dump(self.settings, f)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}