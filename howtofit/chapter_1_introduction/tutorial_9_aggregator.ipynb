{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 8: Aggregator\n",
        "======================\n",
        "\n",
        "In the previous tutorial, we fitted 3 datasets with an identical `NonLinearSearch`, outputting the results of each to a\n",
        "unique folder on our hard disk.\n",
        "\n",
        "In this tutorial, we'll use the `Aggregator` to load the `Result`'s and manipulate / plot them using our Jupyter\n",
        "notebook. The API for using `Result`'s follow closely tutorial 1 of this chapter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the results of the previous tutorial into the `Aggregator`, we simply point the `Aggregator` class to the path \n",
        "of the results we want it to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg = af.Aggregator(\n",
        "    directory=path.join(\"output\", \"howtofit\", \"chapter_1\", \"aggregator\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, let me quickly explain what a generator is in Python, for those unaware. A generator is an object that \n",
        "iterates over a function when it is called. The `Aggregator` creates all objects as generators, rather than lists, or \n",
        "dictionaries, or whatever.\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many `Dataset`s, you'll \n",
        "have lots of results and therefore use a lot of memory. This will crash your laptop! On the other hand, a generator \n",
        "only stores the object in memory when it runs the function; it is free to overwrite it afterwards. Thus, your laptop \n",
        "won't crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        " 1) A generator has no length, thus to determine how many entries of data it corresponds to you first must turn it to a \n",
        "    list.\n",
        "\n",
        " 2) Once we use a generator, we cannot use it again and we'll need to remake it. For this reason, we typically avoid \n",
        "    storing the generator as a variable and instead use the `Aggregator` to create them on use.\n",
        "\n",
        "We can now create a `Samples` generator of every fit. This creates instances of the `Samples` class we manipulated in\n",
        "tutorial 1, which with the `Aggregator` now acts as an interface between the results of the non-linear fit on your \n",
        "hard-disk and Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this list of outputs you should see over 3 different `NestSamples` instances, corresponding to the 3\n",
        "model-fits we performed in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've encountered the `Samples` class in previous tutorials. As we saw in tutorial 1, the `Samples` class contains all \n",
        "the accepted parameter samples of the `NonLinearSearch`, which is a list of lists where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit.\n",
        "\n",
        "With the `Aggregator` we can now get information on the `Samples` of all 3 model-fits, as opposed to just 1 fit using \n",
        "its `Result` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"All parameters of the very first sample\")\n",
        "    print(samples.parameters[0])\n",
        "    print(\"The tenth sample`s third parameter\")\n",
        "    print(samples.parameters[9][2])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `Aggregator` to get information on the `log_likelihoods`, log_priors`, `weights`, etc. of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "    print(samples.log_likelihoods[9])\n",
        "    print(samples.log_priors[9])\n",
        "    print(samples.log_posteriors[9])\n",
        "    print(samples.weights[9])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `Sample`'s to create a list of the `max_log_likelihood_vector` of each fit to our three images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vector = [samps.max_log_likelihood_vector for samps in agg.values(\"samples\")]\n",
        "print(\"Maximum Log Likelihood Parameter Lists:\\n\")\n",
        "print(vector, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As discussed in tutorial 1, using vectors isn't too much use, as we can`t be sure which values correspond to which \n",
        "parameters.\n",
        "\n",
        "We can use the `Aggregator` to create the `max_log_likelihood_instance` of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instances = [samps.max_log_likelihood_instance for samps in agg.values(\"samples\")]\n",
        "print(\"Maximum Log Likelihood Model Instances:\\n\")\n",
        "print(instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model instance contains all the model components of our fit which for the fits above was a single `Gaussian`\n",
        "profile (the word `gaussian` comes from what we called it in the `CollectionPriorModel` above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instances[0].gaussian)\n",
        "print(instances[1].gaussian)\n",
        "print(instances[2].gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This, of course, gives us access to any individual parameter of our maximum log likelihood `instance`. Below, we see \n",
        "that the 3 `Gaussian`s were simulated using `sigma` values of 1.0, 5.0 and 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instances[0].gaussian.sigma)\n",
        "print(instances[1].gaussian.sigma)\n",
        "print(instances[2].gaussian.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also access the `median_pdf` model via the `Aggregator`, as we saw for the `Samples` object in tutorial 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_vectors = [samps.median_pdf_vector for samps in agg.values(\"samples\")]\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "\n",
        "print(\"Median PDF Model Parameter Lists:\\n\")\n",
        "print(mp_vectors, \"\\n\")\n",
        "print(\"Most probable Model Instances:\\n\")\n",
        "print(mp_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also print the `model_results` of all phases, which is string that summarizes every fit`s model providing\n",
        "quick inspection of all results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = agg.model_results\n",
        "print(\"Model Results Summary:\\n\")\n",
        "print(results, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets end the tutorial with something more ambitious. Lets create a plot of the inferred `sigma` values vs `intensity` \n",
        "of each `Gaussian` profile, including error bars at $3\\sigma$ confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "ue3_instances = [\n",
        "    samp.error_instance_at_upper_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    samp.error_instance_at_lower_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "mp_sigmas = [instance.gaussian.sigma for instance in mp_instances]\n",
        "ue3_sigmas = [instance.gaussian.sigma for instance in ue3_instances]\n",
        "le3_sigmas = [instance.gaussian.sigma for instance in le3_instances]\n",
        "mp_intensitys = [instance.gaussian.sigma for instance in mp_instances]\n",
        "ue3_intensitys = [instance.gaussian.sigma for instance in ue3_instances]\n",
        "le3_intensitys = [instance.gaussian.intensity for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=mp_sigmas,\n",
        "    y=mp_intensitys,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    xerr=[le3_sigmas, ue3_sigmas],\n",
        "    yerr=[le3_intensitys, ue3_intensitys],\n",
        ")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}