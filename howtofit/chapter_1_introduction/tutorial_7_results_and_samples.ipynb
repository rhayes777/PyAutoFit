{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 6: Results And Samples\n",
        "===============================\n",
        "\n",
        "In this tutorial, we'll cover all of the output that comes from a `NonLinearSearch` in the form of a `Result` object.\n",
        "\n",
        "We used this object at various points in the chapter, and the bulk of material covered here is described in the example\n",
        "script `autofit_workspace/examples/simple/result.py`. Nevertheless, it is a good idea to refresh ourselves about how\n",
        "results in **PyAutoFit** work before covering more advanced material."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"howtofit\", \"chapter_1\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll reuse the `plot_line` and `Analysis` classes of the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_line(\n",
        "    xvalues,\n",
        "    line,\n",
        "    title=None,\n",
        "    ylabel=None,\n",
        "    errors=None,\n",
        "    color=\"k\",\n",
        "    output_path=None,\n",
        "    output_filename=None,\n",
        "):\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if not path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [profile.profile_from_xvalues(xvalues=xvalues) for profile in instance]\n",
        "        )\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\"\"\"\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n",
        "\n",
        "\n",
        "\"\"\"Now lets run the `NonLinearSearch` to get ourselves a `Result`.\"\"\"\n",
        "\n",
        "import profiles as p\n",
        "\n",
        "model = af.CollectionPriorModel(gaussian=p.Gaussian)\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "emcee = af.Emcee(\n",
        "    name=\"tutorial_6__results_and_samples\", path_prefix=\"howtofit/chapter_1\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \\n\"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/chapter_1/tutorial_6__results_and_samples \\n\"\n",
        "    \"folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Here, we'll look in detail at what information is contained in the `Result`.\n",
        "\n",
        "It contains a `Samples` object, which contains information on the `NonLinearSearch`, for example the parameters. \n",
        "\n",
        "The parameters are stored as a list of lists, where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "print(\"All Parameters:\")\n",
        "print(samples.parameters)\n",
        "print(\"Sample 10`s third parameter value (Gaussian -> sigma)\")\n",
        "print(samples.parameters[9][1], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class also contains the `log_likelihoods`, `log_priors`, `log_posteriors` and `weights` of every \n",
        "accepted sample, where:\n",
        "\n",
        " - A `log_likelihood` is the value evaluated from the `log_likelihood_function` (e.g. -0.5 * `chi_squared` + the \n",
        " `noise_normalization`).\n",
        "\n",
        " - The `log_prior` encodes information on how the priors on the parameters maps the `log_likelihood` value to the \n",
        "  `log_posterior` value.\n",
        "\n",
        " - The `log_posterior` is `log_likelihood` + `log_prior`.\n",
        "\n",
        " - The `weights` gives information on how samples should be combined to estimate the posterior. The values \n",
        "   depend on the `NonLinearSearch` used, for MCMC samples they are all 1 (e.g. all weighted equally)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"All Log Likelihoods:\")\n",
        "print(samples.log_likelihoods)\n",
        "print(\"All Log Priors:\")\n",
        "print(samples.log_priors)\n",
        "print(\"All Log Posteriors:\")\n",
        "print(samples.log_posteriors)\n",
        "print(\"All Sample Weights:\")\n",
        "print(samples.weights, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` contain many useful vectors, including the the maximum log likelihood and posterior values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_log_likelihood_vector = samples.max_log_likelihood_vector\n",
        "max_log_posterior_vector = samples.max_log_posterior_vector\n",
        "\n",
        "print(\"Maximum Log Likelihood Vector:\")\n",
        "print(max_log_likelihood_vector)\n",
        "print(\"Maximum Log Posterior Vector:\")\n",
        "print(max_log_posterior_vector, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This provides us with lists of all model parameters. However, this isn't that much use, which values correspond to \n",
        "which parameters?\n",
        "\n",
        "The list of parameter names are available as a property of the `Model` included with the `Samples`, as are labels \n",
        "which can be used for labeling figures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = samples.model\n",
        "print(model)\n",
        "print(model.parameter_names)\n",
        "print(model.parameter_labels)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is more useful to return the `Result`'s as an `instance`, which is an instance of the `model` using the Python \n",
        "classes used to compose it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_log_likelihood_instance = samples.max_log_likelihood_instance"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `model instance` contains all the model components of our fit, which for the fit above was a single `Gaussian`\n",
        "profile (the word `gaussian` comes from what we called it in the `CollectionPriorModel` above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(max_log_likelihood_instance.gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can unpack the parameters of the `Gaussian` to reveal the `max_log_likelihood_instance`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", max_log_likelihood_instance.gaussian.centre)\n",
        "print(\"Intensity = \", max_log_likelihood_instance.gaussian.intensity)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.gaussian.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our example problem of fitting a 1D `Gaussian` profile, this makes it straight forward to plot the \n",
        "`max_log_likelihood_instance`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = samples.max_log_likelihood_instance.gaussian.profile_from_xvalues(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(data.shape[0]), data)\n",
        "plt.plot(range(data.shape[0]), model_data)\n",
        "plt.title(\"Illustrative model fit to 1D `Gaussian` profile data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile intensity\")\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also access the `median PDF` model, which is the model computed by marginalizing over the samples of every \n",
        "parameter in 1D and taking the median of this PDF.\n",
        "\n",
        "The `median_pdf_vector` is readily available from the `Samples` object for you convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "median_pdf_vector = samples.median_pdf_vector\n",
        "print(\"Median PDF Vector:\")\n",
        "print(median_pdf_vector, \"\\n\")\n",
        "\n",
        "median_pdf_instance = samples.median_pdf_instance\n",
        "print(\"Median PDF `Gaussian` Instance:\")\n",
        "print(\"Centre = \", median_pdf_instance.gaussian.centre)\n",
        "print(\"Intensity = \", median_pdf_instance.gaussian.intensity)\n",
        "print(\"Sigma = \", median_pdf_instance.gaussian.sigma, \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class also provides methods for computing the error estimates of all parameters at an input sigma \n",
        "confidence limit, which can be returned as the values of the parameters including their errors or the size of the \n",
        "errors on each parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vector_at_upper_sigma = samples.vector_at_upper_sigma(sigma=3.0)\n",
        "vector_at_lower_sigma = samples.vector_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Parameter values w/ error (at 3.0 sigma confidence):\")\n",
        "print(vector_at_upper_sigma)\n",
        "print(\"lower Parameter values w/ errors (at 3.0 sigma confidence):\")\n",
        "print(vector_at_lower_sigma, \"\\n\")\n",
        "\n",
        "error_vector_at_upper_sigma = samples.error_vector_at_upper_sigma(sigma=3.0)\n",
        "error_vector_at_lower_sigma = samples.error_vector_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Error values (at 3.0 sigma confidence):\")\n",
        "print(error_vector_at_upper_sigma)\n",
        "print(\"lower Error values (at 3.0 sigma confidence):\")\n",
        "print(error_vector_at_lower_sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All methods above are available as an `instance`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_at_upper_sigma = samples.instance_at_upper_sigma\n",
        "instance_at_lower_sigma = samples.instance_at_lower_sigma\n",
        "error_instance_at_upper_sigma = samples.error_instance_at_upper_sigma\n",
        "error_instance_at_lower_sigma = samples.error_instance_at_lower_sigma"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An `instance` of any accepted parameter space sample can be created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.instance_from_sample_index(sample_index=500)\n",
        "print(\"Gaussian Instance of sample 5000:\")\n",
        "print(\"Centre = \", instance.gaussian.centre)\n",
        "print(\"Intensity = \", instance.gaussian.intensity)\n",
        "print(\"Sigma = \", instance.gaussian.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because `Emcee`, an MCMC *NonLinearSearch* was used, the `log_evidence` of the model is not available. However, if a \n",
        "nested sampling algorithm such as `DynestyStatic` had been used the Bayesian evidence would of been available to \n",
        "perform model (the log evidence below gives `None`).."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF`s) of the results can be plotted using the library:\n",
        "\n",
        " `corner.py`: https://corner.readthedocs.io/en/latest/\n",
        "\n",
        "(In built visualization for PDF`s and `NonLinearSearch`'s is a future feature of **PyAutoFit**, but for now you'll \n",
        "have to use the libraries yourself!).\n",
        "\"\"\"\n",
        "\n",
        "import corner\n",
        "\n",
        "corner.corner(\n",
        "    xs=samples.parameters,\n",
        "    weights=samples.weights,\n",
        "    labels=samples.model.parameter_labels,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}