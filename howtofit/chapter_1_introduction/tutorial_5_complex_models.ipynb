{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Complex Models\n",
        "==========================\n",
        "\n",
        "Up to now, we've fitted a very simple model, a 1D `Gaussian` with 3 free parameters. In this tutorial, we'll look at\n",
        "how **PyAutoFit** allows us to compose and fit models of arbitrary complexity.\n",
        "\n",
        "To begin, you should check out the module `autofit_workspace/howtofit/chapter_1_introduction/profiles.py`.\n",
        "\n",
        "In previous tutorials we used the module `gaussian.py` which contained only the `Gaussian` class. The `profiles.py`\n",
        "includes a second profile, `Exponential`, which like the `Gaussian` class is a model-component that can be fitted to\n",
        "data.\n",
        "\n",
        "Up to now, our data has always been generated using a single `Gaussian` profile. Thus, we have only needed to fit\n",
        "it with a single `Gaussian`. In this tutorial, our `dataset` is now a superpositions of multiple profiles. The models\n",
        "we compose and fit are therefore composed of multiple profiles, such that when we generate the model-data we\n",
        "generate it as the sum of all individual profiles in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets quickly recap tutorial 1, where using `PriorModels` we created a `Gaussian` as a model component and used it to \n",
        "map a list of parameters to a model `instance`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import profiles as p\n",
        "\n",
        "model = af.PriorModel(p.Gaussian)\n",
        "\n",
        "print(\"PriorModel `Gaussian` object: \\n\")\n",
        "print(model)\n",
        "\n",
        "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3])\n",
        "\n",
        "print(\"Model Instance: \\n\")\n",
        "print(instance)\n",
        "\n",
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x = \", instance.centre)\n",
        "print(\"intensity = \", instance.intensity)\n",
        "print(\"sigma = \", instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining a model using multiple model components is straight forward in **PyAutoFit**, using a `CollectionPriorModel`\n",
        "object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.CollectionPriorModel(gaussian=p.Gaussian, exponential=p.Exponential)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `CollectionPriorModel` behaves like a `PriorModel` but contains a collection of model components. For example, it\n",
        "creates a model instance by mapping a list of parameters, which in this case is 6 (3 for the `Gaussian` (centre,\n",
        "intensity, sigma) and 3 for the `Exponential` (centre, intensity, rate))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `instance` contains each of the model components we defined above, using the input argument name of the\n",
        "`CollectionPriorModel` to define the attributes in the `instance`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x (Gaussian) = \", instance.gaussian.centre)\n",
        "print(\"intensity (Gaussian) = \", instance.gaussian.intensity)\n",
        "print(\"sigma (Gaussian) = \", instance.gaussian.sigma)\n",
        "print(\"x (Exponential) = \", instance.exponential.centre)\n",
        "print(\"intensity (Exponential) = \", instance.exponential.intensity)\n",
        "print(\"sigma (Exponential) = \", instance.exponential.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call the components of a `CollectionPriorModel` whatever we want, and the mapped `instance` will use those names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_custom_names = af.CollectionPriorModel(jammy=p.Gaussian, rich=p.Exponential)\n",
        "\n",
        "instance = model_custom_names.instance_from_vector(\n",
        "    vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01]\n",
        ")\n",
        "\n",
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x (Gaussian) = \", instance.jammy.centre)\n",
        "print(\"intensity (Gaussian) = \", instance.jammy.intensity)\n",
        "print(\"sigma (Gaussian) = \", instance.jammy.sigma)\n",
        "print(\"x (Exponential) = \", instance.rich.centre)\n",
        "print(\"intensity (Exponential) = \", instance.rich.intensity)\n",
        "print(\"sigma (Exponential) = \", instance.rich.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform visualization we'll again use the plot_line function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_line(\n",
        "    xvalues,\n",
        "    line,\n",
        "    title=None,\n",
        "    ylabel=None,\n",
        "    errors=None,\n",
        "    color=\"k\",\n",
        "    output_path=None,\n",
        "    output_filename=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot a 1D line of data on a plot of x versus y, where the x-axis is the x coordinate of the line and the y-axis\n",
        "    is the intensity of the line at that coordinate.\n",
        "\n",
        "    The function include options to output the image to the hard-disk as a .png.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xvalues : np.ndarray\n",
        "        The x-coordinates the profile is defined on.\n",
        "    line : np.ndarray\n",
        "        The intensity values of the profile which are plotted.\n",
        "    ylabel : str\n",
        "        The y-label of the plot.\n",
        "    output_path : str\n",
        "        The path the image is to be output to hard-disk as a .png.\n",
        "    output_filename : str\n",
        "        The filename of the file if it is output as a .png.\n",
        "    output_format : str\n",
        "        Determines where the plot is displayed on your screen (\"show\") or output to the hard-disk as a png (\"png\").\n",
        "    \"\"\"\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if not path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a model composed of multiple components we need to fit it to our data. To do this, we use updated \n",
        "`Analysis` class that creates the `model_data` as a super position of all of the model's individual `Profile`'s. For \n",
        "example, in the model above, the `model_data` is the sum of the `Gaussian`'s  individual profile and `Exponential`'s \n",
        "individual profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of a list of Profiles (Gaussians, Exponentials, etc.) to the dataset, using a\n",
        "        model instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            The list of Profile model instance (e.g. the Gaussians, Exponentials, etc.).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        In tutorials 3 & 4, the instance was an instance of a single `Gaussian` profile. PyAutoFit knew this instance\n",
        "        would contain just one Gaussian, because when the phase was created we used a PriorModel object in PyAutoFit\n",
        "        to make the Gaussian. This meant we could create the model data using the line:\n",
        "\n",
        "            model_data = instance.gaussian.profile_from_xvalues(xvalues=self.masked_dataset.xvalues)\n",
        "\n",
        "        In this tutorial our instance is comprised of multiple Profile objects, because we used a CollectionPriorModel:\n",
        "\n",
        "            model = CollectionPriorModel(gaussian=profiles.Gaussian, exponential=profiles.Exponential).\n",
        "\n",
        "        By using a CollectionPriorModel, this means the instance parameter input into the fit function is a\n",
        "        dictionary where individual profiles (and their parameters) can be accessed as followed:\n",
        "\n",
        "            print(instance.gaussian)\n",
        "            print(instance.exponential)\n",
        "            print(instance.exponential.centre)\n",
        "\n",
        "        The names of the attributes of the instance correspond to what we input into the CollectionPriorModel. Lets\n",
        "        look at a second example:\n",
        "\n",
        "            model = CollectionPriorModel(\n",
        "                          gaussian_0=profiles.Gaussian,\n",
        "                          gaussian_1=profiles.Gaussian,\n",
        "                          whatever_i_want=profiles.Exponential\n",
        "                     ).\n",
        "\n",
        "            print(instance.gaussian_0)\n",
        "            print(instance.gaussian_1)\n",
        "            print(instance.whatever_i_want.centre)\n",
        "\n",
        "        A CollectionPriorModel allows us to name our model components whatever we want!\n",
        "\n",
        "        In this tutorial, we want our `fit` function to fit the data with a profile which is the summed profile\n",
        "        of all individual profiles in the model. Look at `model_data_from_instance` to see how we do this.\n",
        "        \"\"\"\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [profile.profile_from_xvalues(xvalues=xvalues) for profile in instance]\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        For those not familiar with dictionary comprehensions, below I've included how one would use the instance to \n",
        "        create the summed profile using a more simple for loop.\n",
        "\n",
        "            model_data = np.zeros(shape=self.masked_dataset.xvalues.shape[0])\n",
        "\n",
        "            for profile in instance:\n",
        "                model_data += profile.profile_from_xvalues(xvalues=self.masked_dataset.xvalues)\n",
        "\n",
        "            return model_data\n",
        "        \"\"\"\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\"\"\"\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset from the `autofit_workspace/dataset` folder. This uses a new `dataset` that is a sum of a \n",
        "`Gaussian` and `Exponential` profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\n",
        "    \"dataset\", \"howtofit\", \"chapter_1\", \"gaussian_x1__exponential_x1\"\n",
        ")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets now perform the fit using our model which is composed of two `Profile`'s. You'll note that the `Emcee`\n",
        "dimensionality has increased from N=3 to N=6, given that we are now fitting two `Profile`'s each with 3 free parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "emcee = af.Emcee(\n",
        "    name=\"tutorial_5__gaussian_x1__exponential_x1\", path_prefix=\"howtofit/chapter_1\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \\n\"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/chapter_1/tutorial_5__gaussian_x1__exponential_x1 \\n\"\n",
        "    \"folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the results of the fit by going to the folder \n",
        "`autofit_workspace/output/howtofit/chapter_1/tutorial_5__gaussian_x1__exponential_x1`. The fit takes longer to run than \n",
        "the fits performed in previous tutorials, because the dimensionality of the model we fit increases from 3 to 6.\n",
        "\n",
        "With the `CollectionPriorModel`, **PyAutoFit** provides all the tools needed to compose and fit any model imaginable!\n",
        "Lets fit a model composed of two `Gaussian`. and and an `Exponential`, which will have a dimensionality of N=9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\n",
        "    \"dataset\", \"howtofit\", \"chapter_1\", \"gaussian_x2__exponential_x1\"\n",
        ")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "model = af.CollectionPriorModel(\n",
        "    gaussian_0=p.Gaussian, gaussian_1=p.Gaussian, exponential=p.Exponential\n",
        ")\n",
        "\n",
        "emcee = af.Emcee(\n",
        "    name=\"tutorial_5__gaussian_x2__exponential_x1\", path_prefix=\"howtofit/chapter_1\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running.\\n\"\n",
        "    \"checkout the autofit_workspace/output/howtofit/chapter_1/tutorial_5__gaussian_x2__exponential_x1\\n\"\n",
        "    \" folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can fully customize the model that we fit. Lets suppose we have a dataset` that consists of three `Gaussian` \n",
        "profiles, but we also know the following information about the dataset:\n",
        "\n",
        "- All 3 `Gaussian`'s are centrally aligned.\n",
        "- The `sigma` of one `Gaussian` is equal to 1.0.\n",
        "- The sigma of another `Gaussian` is above 3.0.\n",
        "\n",
        "We can edit our `CollectionPriorModel` to meet these constraints accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.CollectionPriorModel(\n",
        "    gaussian_0=p.Gaussian, gaussian_1=p.Gaussian, gaussian_2=p.Gaussian\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This aligns the `centre`'s of the 3 `Gaussian`'s reducing the dimensionality of the model from N=9 to N=7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.gaussian_0.centre = model.gaussian_1.centre\n",
        "model.gaussian_1.centre = model.gaussian_2.centre"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This fixes the `sigma` value of one `Gaussian` to 1.0, further reducing the dimensionality from N=7 to N=6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.gaussian_0.sigma = 1.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This assertion forces all values of the `sigma` value of the third `Gaussian` to  be above 3.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.gaussian_2.add_assertion(model.gaussian_2.sigma > 3.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now fit this model as per usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"howtofit\", \"chapter_1\", \"gaussian_x3\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "emcee = af.Emcee(\n",
        "    name=\"tutorial_5__gaussian_x3\", path_prefix=path.join(\"howtofit\", \"chapter_1\")\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/chapter_1/tutorial_5__gaussian_x3\"\n",
        "    \" folder for live output of the results.\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And with that, we are complete. In this tutorial, we learned how to compose and fit complex models in **PyAutoFit**.\n",
        " \n",
        "To end, you should think again in more detail about your model fitting problem:\n",
        "\n",
        " Are there many different model components you may wish to define and fit?\n",
        "\n",
        " Is your data the super position of many different model components, like the profiles in this tutorial?\n",
        "\n",
        " In this tutorial, all components of our model did the same thing, represent a 1D profile. In your model, you may\n",
        "have model components that represent different parts of your model, which need to be combined in more complicated ways\n",
        "in order to create your model-fit. You now have all the tools you need to define, compose and fit very complex models!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}